{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDS Model Training on Google Colab\n",
        "\n",
        "This notebook trains the ML classification model using data from MongoDB.\n",
        "\n",
        "## Prerequisites\n",
        "- MongoDB database with labeled training data\n",
        "- MongoDB connection string (with network access from Colab)\n",
        "- Sufficient Colab runtime (High-RAM recommended for 8M+ samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IDS Model Training on Google Colab\n",
        "\n",
        "This notebook trains the ML classification model using data from MongoDB.\n",
        "\n",
        "## Prerequisites\n",
        "- MongoDB database with labeled training data\n",
        "- MongoDB connection string (with network access from Colab)\n",
        "- Sufficient Colab runtime (High-RAM recommended for 8M+ samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q pymongo>=4.6.1 pandas>=2.2.0 numpy>=1.26.0 scikit-learn>=1.4.0 tqdm>=4.66.1 matplotlib>=3.8.2 seaborn>=0.13.0 imbalanced-learn>=0.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title MongoDB Configuration\n",
        "# Enter your MongoDB connection string here\n",
        "# For MongoDB Atlas, format: mongodb+srv://username:password@cluster.mongodb.net/\n",
        "# For local MongoDB with port forwarding: mongodb://localhost:27017/\n",
        "\n",
        "MONGODB_URI = \"mongodb://your-connection-string\"  # @param {type:\"string\"}\n",
        "MONGODB_DATABASE_NAME = \"ids_db\"  # @param {type:\"string\"}\n",
        "\n",
        "# Training configuration\n",
        "HYPERPARAMETER_TUNING_ENABLED = True  # @param {type:\"boolean\"}\n",
        "HYPERPARAMETER_TUNING_N_ITER = 20  # @param {type:\"integer\"}\n",
        "HYPERPARAMETER_TUNING_CV = 3  # @param {type:\"integer\"}\n",
        "MODEL_TYPE = \"random_forest\"  # @param [\"random_forest\", \"svm\", \"logistic_regression\"]\n",
        "BATCH_LOADING_ENABLED = True  # @param {type:\"boolean\"}\n",
        "MAX_TRAINING_SAMPLES = None  # @param {type:\"raw\"} Set to None for all samples, or specify a number\n",
        "\n",
        "print(\"✓ Configuration loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Training Module\n",
        "\n",
        "Upload `train_colab.py` to Colab using the file browser, or paste its contents in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Import from uploaded file\n",
        "try:\n",
        "    from train_colab import (\n",
        "        ColabConfig, ColabDataCollector, ColabDataPreprocessor,\n",
        "        ColabClassifier, ColabModelTrainer\n",
        "    )\n",
        "    print(\"✓ Training module loaded from file\")\n",
        "except ImportError:\n",
        "    print(\"⚠ train_colab.py not found. Please upload it or paste the code in the next cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 2: Paste train_colab.py code here if file upload doesn't work\n",
        "# Copy the entire contents of backend/colab/train_colab.py and paste here\n",
        "# Then uncomment the following:\n",
        "\n",
        "# exec(open('train_colab.py').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize Services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create configuration\n",
        "config = ColabConfig(\n",
        "    mongodb_uri=MONGODB_URI,\n",
        "    mongodb_database_name=MONGODB_DATABASE_NAME,\n",
        "    CLASSIFICATION_MODEL_TYPE=MODEL_TYPE,\n",
        "    HYPERPARAMETER_TUNING_ENABLED=HYPERPARAMETER_TUNING_ENABLED,\n",
        "    HYPERPARAMETER_TUNING_N_ITER=HYPERPARAMETER_TUNING_N_ITER,\n",
        "    HYPERPARAMETER_TUNING_CV=HYPERPARAMETER_TUNING_CV,\n",
        "    BATCH_LOADING_ENABLED=BATCH_LOADING_ENABLED,\n",
        "    MAX_TRAINING_SAMPLES=MAX_TRAINING_SAMPLES\n",
        ")\n",
        "\n",
        "# Initialize services\n",
        "print(\"Connecting to MongoDB...\")\n",
        "data_collector = ColabDataCollector(config)\n",
        "preprocessor = ColabDataPreprocessor(config)\n",
        "classifier = ColabClassifier(config)\n",
        "model_trainer = ColabModelTrainer(config, classifier, preprocessor, data_collector)\n",
        "\n",
        "print(\"✓ Services initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Check Database Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get database statistics\n",
        "stats = data_collector.get_statistics()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Database Statistics\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total samples: {stats.get('total_samples', 0):,}\")\n",
        "print(f\"Labeled samples: {stats.get('labeled_samples', 0):,}\")\n",
        "print(f\"Benign: {stats.get('benign_count', 0):,}\")\n",
        "print(f\"Malicious: {stats.get('malicious_count', 0):,}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check memory requirements\n",
        "try:\n",
        "    import psutil\n",
        "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
        "    total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
        "    \n",
        "    labeled_samples = stats.get('labeled_samples', 0)\n",
        "    # Estimate: ~8 bytes per feature value, 81 features\n",
        "    estimated_memory_gb = (labeled_samples * 81 * 8) / (1024**3)\n",
        "    \n",
        "    print(f\"\\nAvailable RAM: {available_memory_gb:.2f} GB\")\n",
        "    print(f\"Total RAM: {total_memory_gb:.2f} GB\")\n",
        "    print(f\"Estimated memory needed: {estimated_memory_gb:.2f} GB\")\n",
        "    \n",
        "    if estimated_memory_gb > available_memory_gb * 0.8:\n",
        "        print(\"\\n⚠ WARNING: Estimated memory usage exceeds 80% of available RAM\")\n",
        "        print(\"Batch loading is recommended (BATCH_LOADING_ENABLED=True)\")\n",
        "    else:\n",
        "        print(\"\\n✓ Memory requirements look good\")\n",
        "except ImportError:\n",
        "    print(\"\\n⚠ psutil not available. Cannot check memory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "print(\"Loading training data from MongoDB...\")\n",
        "print(\"This may take several minutes for large datasets...\")\n",
        "\n",
        "df = model_trainer.load_training_data()\n",
        "\n",
        "print(f\"\\n✓ Loaded {len(df):,} samples\")\n",
        "print(f\"Features: {len(df.columns) - 1}\")  # Exclude label column\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean data\n",
        "print(\"Cleaning data...\")\n",
        "df_clean = preprocessor.clean_data(df)\n",
        "print(f\"✓ Cleaned data: {len(df_clean):,} samples\")\n",
        "\n",
        "# Engineer features\n",
        "print(\"\\nEngineering features...\")\n",
        "df_eng = preprocessor.engineer_features(df_clean)\n",
        "print(f\"✓ Feature engineering complete\")\n",
        "\n",
        "# Split data\n",
        "print(\"\\nSplitting data into train/val/test sets...\")\n",
        "test_size = 1.0 - config.TRAIN_TEST_SPLIT_RATIO\n",
        "val_size = 0.15\n",
        "\n",
        "train_df, val_df, test_df = preprocessor.split_data(\n",
        "    df_eng, test_size=test_size, val_size=val_size, stratify=True\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Data split complete:\")\n",
        "print(f\"  Train: {len(train_df):,} samples\")\n",
        "print(f\"  Validation: {len(val_df):,} samples\")\n",
        "print(f\"  Test: {len(test_df):,} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "print(\"=\"*60)\n",
        "print(\"Starting Model Training\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if HYPERPARAMETER_TUNING_ENABLED:\n",
        "    print(f\"Hyperparameter tuning: ENABLED\")\n",
        "    print(f\"  Iterations: {HYPERPARAMETER_TUNING_N_ITER}\")\n",
        "    print(f\"  CV folds: {HYPERPARAMETER_TUNING_CV}\")\n",
        "    print(f\"\\n⚠ This will take several hours for large datasets...\")\n",
        "else:\n",
        "    print(\"Hyperparameter tuning: DISABLED\")\n",
        "    print(\"Using default hyperparameters\")\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "start_time = datetime.now()\n",
        "\n",
        "training_results = model_trainer.train_model(\n",
        "    hyperparameter_tuning=HYPERPARAMETER_TUNING_ENABLED\n",
        ")\n",
        "\n",
        "end_time = datetime.now()\n",
        "training_duration = (end_time - start_time).total_seconds() / 3600  # hours\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training time: {training_duration:.2f} hours\")\n",
        "print(f\"Training samples: {training_results['training_samples']:,}\")\n",
        "print(f\"Validation samples: {training_results['validation_samples']:,}\")\n",
        "print(f\"Test samples: {training_results['test_samples']:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display evaluation metrics\n",
        "test_metrics = training_results['test_metrics']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Test Set Evaluation Metrics\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:  {test_metrics.get('accuracy', 0):.4f}\")\n",
        "print(f\"Precision: {test_metrics.get('precision', 0):.4f}\")\n",
        "print(f\"Recall:    {test_metrics.get('recall', 0):.4f}\")\n",
        "print(f\"F1-Score:  {test_metrics.get('f1_score', 0):.4f}\")\n",
        "print(f\"ROC-AUC:   {test_metrics.get('roc_auc', 0):.4f}\")\n",
        "\n",
        "# Display confusion matrix\n",
        "cm = test_metrics.get('confusion_matrix', [])\n",
        "if cm:\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(f\"  True Negatives (Benign):  {cm[0][0]:,}\")\n",
        "    print(f\"  False Positives:          {cm[0][1]:,}\")\n",
        "    print(f\"  False Negatives:          {cm[1][0]:,}\")\n",
        "    print(f\"  True Positives (Malicious): {cm[1][1]:,}\")\n",
        "\n",
        "# Display hyperparameters if tuning was performed\n",
        "if training_results.get('hyperparameters'):\n",
        "    print(\"\\nBest Hyperparameters:\")\n",
        "    for param, value in training_results['hyperparameters'].items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "model_filename = 'classification_model.pkl'\n",
        "\n",
        "print(f\"Saving model to {model_filename}...\")\n",
        "model_path = model_trainer.save_model(model_filename)\n",
        "\n",
        "# Check file size\n",
        "file_size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "print(f\"\\n✓ Model saved successfully\")\n",
        "print(f\"File: {model_path}\")\n",
        "print(f\"Size: {file_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download model file\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading model file...\")\n",
        "files.download(model_filename)\n",
        "\n",
        "print(\"\\n✓ Model download initiated\")\n",
        "print(\"\\nAfter downloading, replace your local classification_model.pkl file\")\n",
        "print(\"in the backend/ directory with this downloaded file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Optional: Save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Save model to Google Drive\n",
        "# Uncomment and run this cell if you want to save to Drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# \n",
        "# import shutil\n",
        "# drive_path = '/content/drive/MyDrive/IDS_Model'\n",
        "# os.makedirs(drive_path, exist_ok=True)\n",
        "# \n",
        "# shutil.copy(model_filename, drive_path)\n",
        "# print(f\"✓ Model saved to Google Drive: {drive_path}/{model_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Training completed successfully! The model file has been downloaded.\n",
        "\n",
        "### Next Steps:\n",
        "1. Replace `backend/classification_model.pkl` with the downloaded file\n",
        "2. Restart your Flask backend to load the new model\n",
        "3. Test the model using the Analysis page in the frontend\n",
        "\n",
        "### Notes:\n",
        "- Training time depends on dataset size and hyperparameter tuning settings\n",
        "- For 8M+ samples with hyperparameter tuning, expect 4-8 hours\n",
        "- Colab free tier has 12-hour session limits\n",
        "- Consider using Colab Pro for longer training sessions"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
